{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS4AEZCoJgRp"
      },
      "source": [
        "**Get the environment properties**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktwmsGyOJeYn",
        "outputId": "bf99bbf7-3c6f-443a-9168-9ec9099d1223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t---Get distro and release information---\n",
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.6 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n",
            "\n",
            "\t---Get cpu properties---\n",
            "CPU(s):              2\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "\n",
            "\t---Get available memory---\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        1.3G        8.9G        1.3M        2.5G         11G\n",
            "Swap:            0B          0B          0B\n",
            "\n",
            "\t---Get file system attributes---\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   25G   84G  23% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.1G  910M  54% /sbin/docker-init\n",
            "tmpfs           6.4G   72K  6.4G   1% /var/colab\n",
            "/dev/sda1        41G   27G   14G  67% /etc/hosts\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ]
        }
      ],
      "source": [
        "print(\"\\t---Get distro and release information---\")\n",
        "!lsb_release -a \n",
        "print(\"\\n\\t---Get cpu properties---\")\n",
        "!lscpu | grep -E '^Model name|^Thread|^Core|^Socket|^CPU\\('\n",
        "print(\"\\n\\t---Get available memory---\")\n",
        "!free -m -h \n",
        "print(\"\\n\\t---Get file system attributes---\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cK83SZZLLpX"
      },
      "source": [
        "**Requirements installation**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZW9I9PYK7Zw",
        "outputId": "9c953c83-7721-4474-eff3-e8018eeff8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version: \n",
            "openjdk version \"11.0.16\" 2022-07-19\n",
            "OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu118.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu118.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!pip -qq install findspark\n",
        "print(\"Java version: \")\n",
        "!java -version\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "!pip -qq install pyspark\n",
        "!pip -qq install geopandas attrs shapely rtree apache-sedona[spark]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ovQA9-PSDT"
      },
      "source": [
        "**Get data from drive**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVz7RO6lLh61",
        "outputId": "caa41d6e-bb20-4e45-d279-9b339e7880a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QztphAhscVlaY5vB2s6b2LuCiTzDp39N\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/3.60M [00:00<?, ?B/s]\r100% 3.60M/3.60M [00:00<00:00, 173MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: data/CottonYield_1980-2020.csv  \n",
            "  inflating: data/SurfaceWaterQuality.csv  \n",
            "   creating: data/toxicityData/\n",
            "  inflating: data/toxicityData/chihuahua_data.csv  \n",
            "  inflating: data/toxicityData/chihuahua_site.csv  \n",
            "  inflating: data/toxicityData/coahuila_data.csv  \n",
            "  inflating: data/toxicityData/coahuila_site.csv  \n",
            "  inflating: data/toxicityData/durango_data.csv  \n",
            "  inflating: data/toxicityData/durango_site.csv  \n",
            "  inflating: data/toxicityData/nuevoleon_data.csv  \n",
            "  inflating: data/toxicityData/nuevoleon_sites.csv  \n",
            "  inflating: data/toxicityData/simbology.csv  \n",
            "  inflating: data/toxicityData/sinaloa_data.csv  \n",
            "  inflating: data/toxicityData/sinaloa_site.csv  \n",
            "  inflating: data/toxicityData/sonora_data.csv  \n",
            "  inflating: data/toxicityData/sonora_site.csv  \n",
            "  inflating: data/toxicityData/tamaulipas_data.csv  \n",
            "  inflating: data/toxicityData/tamaulipas_site.csv  \n",
            "  inflating: data/UndergroundWaterQuality.csv  \n"
          ]
        }
      ],
      "source": [
        "#gets data from csv files in google and puts them in a data folder\n",
        "!gdown --fuzzy 'https://drive.google.com/file/d/1QztphAhscVlaY5vB2s6b2LuCiTzDp39N/view?usp=share_link' -O /content/\n",
        "!unzip -o data.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGBcdoasUJjm"
      },
      "source": [
        "**Setup Apache Spark and Sedona**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVHFqTj1UlKx",
        "outputId": "01a1ce25-3bf2-49f2-bb26-604e581ac090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import shapely\n",
        "import pandas as pd \n",
        "import geopandas as gpd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from sedona.register import SedonaRegistrator\n",
        "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
        "spark = SparkSession. \\\n",
        "builder. \\\n",
        "appName('GeoBigData'). \\\n",
        "config(\"spark.serializer\", KryoSerializer.getName). \\\n",
        "config(\"spark.executor.memory\", \"5g\"). \\\n",
        "config(\"spark.driver.memory\", \"10g\"). \\\n",
        "config('spark.driver.maxResultSize', '5g'). \\\n",
        "config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
        "config('spark.jars.packages',\n",
        "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.0-incubating,'\n",
        "           'org.datasyslab:geotools-wrapper:1.1.0-25.2'). \\\n",
        "getOrCreate()\n",
        "SedonaRegistrator.registerAll(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjrrBukPU3DI"
      },
      "source": [
        "**Load DB**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the csv files into databases with spark for later use"
      ],
      "metadata": {
        "id": "tCl-MsRibR_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSqQT0TKUrN_",
        "outputId": "442e60c1-f3fb-483b-b557-dc3ef646f029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- C_SITIO: string (nullable = true)\n",
            " |-- C_MONITOREO: string (nullable = true)\n",
            " |-- FECHA: string (nullable = true)\n",
            " |-- AS_SOL: string (nullable = true)\n",
            "\n",
            "+--------------+--------------------+---------+------+\n",
            "|       C_SITIO|         C_MONITOREO|    FECHA|AS_SOL|\n",
            "+--------------+--------------------+---------+------+\n",
            "|OCGNO0001RNL21|OCGNO0001RNL21-30...|4/30/2021|  null|\n",
            "|OCGNO0002RNL21|OCGNO0002RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0003RNL21|OCGNO0003RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0004RNL21|OCGNO0004RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0005RNL21|OCGNO0005RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0006RNL21|OCGNO0006RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0007RNL21|OCGNO0007RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0008RNL21|OCGNO0008RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0009RNL21|OCGNO0009RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0010RNL21|OCGNO0010RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0011RNL21|OCGNO0011RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0012RNL21|OCGNO0012RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0013RNL21|OCGNO0013RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0014RNL21|OCGNO0014RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0015RNL21|OCGNO0015RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0016RNL21|OCGNO0016RNL21-20...|5/20/2021|  null|\n",
            "|OCGNO0017RNL21|OCGNO0017RNL21-30...|4/30/2021|  null|\n",
            "|OCGNO0018RNL21|OCGNO0018RNL21-30...|4/30/2021|  null|\n",
            "|OCGNO0019RNL21|OCGNO0019RNL21-30...|4/30/2021|  null|\n",
            "|OCGNO0020RNL21|OCGNO0020RNL21-30...|4/30/2021|  null|\n",
            "+--------------+--------------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DB_SWQ = spark.read.option(\"header\",True).csv(f\"/content/data/SurfaceWaterQuality.csv\")\n",
        "DB_SWQ_Q = DB_SWQ.select('CLAVE','SITIO','ESTADO', 'TOX_D_48_UT', 'CALIDAD_TOX_D_48')\n",
        "DB_SWQ_Q.cache()\n",
        "DB_cotton = spark.read.option(\"header\",True).csv(f\"/content/data/CottonYield_1980-2020.csv\")\n",
        "DB_Cotton_Q = DB_cotton.select('Ano','Estado','Ciclo', 'Modalidad', 'Produccion','Superficie')\n",
        "DB_Cotton_Q.cache()\n",
        "DB_ToxChi = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/chihuahua_data.csv\")\n",
        "DB_ToxChi_Q = DB_ToxChi.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxChi_Q.cache()\n",
        "DB_ToxCoa = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/coahuila_data.csv\")\n",
        "DB_ToxCoa_Q = DB_ToxCoa.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxCoa_Q.cache()\n",
        "DB_ToxDur = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/durango_data.csv\")\n",
        "DB_ToxDur_Q = DB_ToxDur.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxDur_Q.cache()\n",
        "DB_ToxNL = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/nuevoleon_data.csv\")\n",
        "DB_ToxNL_Q = DB_ToxNL.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxNL_Q.cache()\n",
        "DB_ToxSin = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/sinaloa_data.csv\")\n",
        "DB_ToxSin_Q = DB_ToxSin.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxSin_Q.cache()\n",
        "DB_ToxSon = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/sonora_data.csv\")\n",
        "DB_ToxSon_Q = DB_ToxSon.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxSon_Q.cache()\n",
        "DB_ToxTam = spark.read.option(\"header\",True).csv(f\"/content/data/toxicityData/tamaulipas_data.csv\")\n",
        "DB_ToxTam_Q = DB_ToxTam.select('C_SITIO', 'C_MONITOREO', 'FECHA', 'AS_SOL')\n",
        "DB_ToxTam_Q.cache()\n",
        "DB_ToxTam_Q.printSchema()\n",
        "DB_ToxTam_Q.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select valid values"
      ],
      "metadata": {
        "id": "uqMDEY6cbIGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DB_ToxChi_Q.createOrReplaceTempView(\"ToxChi\")\n",
        "DB_ToxChi_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxChi where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxChi_Qf.cache()\n",
        "DB_ToxChi_Qf.show()\n",
        "DB_ToxCoa_Q.createOrReplaceTempView(\"ToxCoa\")\n",
        "DB_ToxCoa_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxCoa where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxCoa_Qf.cache()\n",
        "DB_ToxDur_Q.createOrReplaceTempView(\"ToxDur\")\n",
        "DB_ToxDur_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxDur where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxDur_Qf.cache()\n",
        "DB_ToxNL_Q.createOrReplaceTempView(\"ToxNL\")\n",
        "DB_ToxNL_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxNL where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxNL_Qf.cache()\n",
        "DB_ToxSin_Q.createOrReplaceTempView(\"ToxSin\")\n",
        "DB_ToxSin_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxSin where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxSin_Qf.cache()\n",
        "DB_ToxSon_Q.createOrReplaceTempView(\"ToxSon\")\n",
        "DB_ToxSon_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxSon where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxSon_Qf.cache()\n",
        "DB_ToxTam_Q.createOrReplaceTempView(\"ToxTam\")\n",
        "DB_ToxTam_Qf = spark.sql(\"\"\"select C_SITIO, C_MONITOREO, FECHA, if(AS_SOL = '<0.01', 0.005, AS_SOL) as AS_SOL from ToxTam where !isnull(AS_SOL)\"\"\")\n",
        "DB_ToxTam_Qf.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHiFs6SrZbYK",
        "outputId": "d4ccdf2a-ef1b-42bc-9215-8de35400e51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+----------+------+\n",
            "|   C_SITIO|      C_MONITOREO|     FECHA|AS_SOL|\n",
            "+----------+-----------------+----------+------+\n",
            "|  DLCHI291|  DLCHI291-120921|09/07/2021|0.0247|\n",
            "|  DLCHI296|  DLCHI296-270920| 9/22/2020| 0.005|\n",
            "|  DLCHI296|  DLCHI296-120921|09/07/2021| 0.005|\n",
            "|  DLCHI297|  DLCHI297-270920| 9/22/2020| 0.005|\n",
            "|  DLCHI297|  DLCHI297-120921|09/07/2021| 0.005|\n",
            "|  DLCHI300|  DLCHI300-270920| 9/22/2020| 0.005|\n",
            "|  DLCHI301|  DLCHI301-270920| 9/21/2020| 0.005|\n",
            "|  DLCHI301|  DLCHI301-120921|09/07/2021| 0.005|\n",
            "|  DLCHI307|  DLCHI307-041020|10/04/2020|0.0239|\n",
            "|  DLCHI308|  DLCHI308-120921|09/06/2021|0.0157|\n",
            "|  DLCHI309|  DLCHI309-041020| 9/25/2020| 0.005|\n",
            "|  DLCHI311|  DLCHI311-041020| 9/24/2020| 0.005|\n",
            "|  DLCHI315|  DLCHI315-111020|10/11/2020| 0.005|\n",
            "|  DLCHI316|  DLCHI316-111020|10/11/2020| 0.005|\n",
            "|  DLCHI318|  DLCHI318-111020|10/11/2020| 0.005|\n",
            "|DLCHI326M1|DLCHI326M1-270920| 9/18/2020|0.3935|\n",
            "|DLCHI327M1|DLCHI327M1-270920| 9/18/2020| 0.005|\n",
            "|  DLCHI329|  DLCHI329-120921|09/08/2021| 0.005|\n",
            "|  DLCHI330|  DLCHI330-111020|10/12/2020|0.0151|\n",
            "|  DLCHI330|  DLCHI330-120921|09/08/2021|0.0145|\n",
            "+----------+-----------------+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[C_SITIO: string, C_MONITOREO: string, FECHA: string, AS_SOL: string]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}